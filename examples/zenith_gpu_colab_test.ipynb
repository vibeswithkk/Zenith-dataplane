{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "#  Zenith GPU Runtime - Colab Test\n",
                "\n",
                "**Test Zenith's GPU capabilities on Google Colab with real hardware.**\n",
                "\n",
                "This notebook tests:\n",
                "1. CUDA device detection\n",
                "2. GPU memory management\n",
                "3. Multi-GPU topology (if available)\n",
                "4. NVML monitoring\n",
                "5. Integration with PyTorch/TensorFlow\n",
                "\n",
                "---\n",
                "\n",
                "**[!] Make sure to select GPU runtime:**\n",
                "- Go to `Runtime` → `Change runtime type` → `T4 GPU` or `A100`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "check_gpu"
            },
            "outputs": [],
            "source": [
                "#@title 1. Check GPU Availability\n",
                "import subprocess\n",
                "import os\n",
                "\n",
                "def check_gpu():\n",
                "    \"\"\"Check if GPU is available.\"\"\"\n",
                "    try:\n",
                "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
                "        if result.returncode == 0:\n",
                "            print(\"[OK] GPU detected!\")\n",
                "            print(result.stdout)\n",
                "            return True\n",
                "        else:\n",
                "            print(\"[FAIL] No GPU detected\")\n",
                "            return False\n",
                "    except FileNotFoundError:\n",
                "        print(\"[FAIL] nvidia-smi not found\")\n",
                "        return False\n",
                "\n",
                "GPU_AVAILABLE = check_gpu()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "#@title 2. Install Dependencies\n",
                "!pip install -q pynvml torch\n",
                "print(\"[OK] Dependencies installed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clone_repo"
            },
            "outputs": [],
            "source": [
                "#@title 3. Clone Zenith Repository\n",
                "import os\n",
                "\n",
                "if not os.path.exists('Zenith-dataplane'):\n",
                "    !git clone --depth 1 https://github.com/vibeswithkk/Zenith-dataplane.git\n",
                "    print(\"[OK] Repository cloned\")\n",
                "else:\n",
                "    !cd Zenith-dataplane && git pull\n",
                "    print(\"[OK] Repository updated\")\n",
                "\n",
                "os.chdir('Zenith-dataplane')\n",
                "print(f\"Working directory: {os.getcwd()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section_nvml"
            },
            "source": [
                "##  GPU Detection & NVML Monitoring\n",
                "\n",
                "Test GPU device detection similar to `zenith-runtime-gpu/src/nvml.rs`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_nvml"
            },
            "outputs": [],
            "source": [
                "#@title 4. NVML Device Detection (Python equivalent of nvml.rs)\n",
                "import pynvml\n",
                "from dataclasses import dataclass\n",
                "from typing import List, Dict, Optional\n",
                "\n",
                "@dataclass\n",
                "class GpuDevice:\n",
                "    \"\"\"GPU device info - mirrors Rust GpuDevice struct.\"\"\"\n",
                "    device_id: int\n",
                "    name: str\n",
                "    uuid: str\n",
                "    total_memory: int  # bytes\n",
                "    free_memory: int\n",
                "    used_memory: int\n",
                "    temperature: int  # celsius\n",
                "    utilization: int  # percent\n",
                "    power_usage: float  # watts\n",
                "    compute_capability: tuple\n",
                "\n",
                "class NvmlMonitor:\n",
                "    \"\"\"\n",
                "    Python equivalent of zenith-runtime-gpu's NVML wrapper.\n",
                "    Tests the same functionality as nvml.rs.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        pynvml.nvmlInit()\n",
                "        self.device_count = pynvml.nvmlDeviceGetCount()\n",
                "        print(f\"[OK] NVML initialized: {self.device_count} GPU(s) detected\")\n",
                "    \n",
                "    def get_device(self, device_id: int) -> GpuDevice:\n",
                "        \"\"\"Get GPU device info.\"\"\"\n",
                "        handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)\n",
                "        \n",
                "        name = pynvml.nvmlDeviceGetName(handle)\n",
                "        uuid = pynvml.nvmlDeviceGetUUID(handle)\n",
                "        mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
                "        \n",
                "        try:\n",
                "            temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
                "        except:\n",
                "            temp = 0\n",
                "            \n",
                "        try:\n",
                "            util = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
                "            gpu_util = util.gpu\n",
                "        except:\n",
                "            gpu_util = 0\n",
                "            \n",
                "        try:\n",
                "            power = pynvml.nvmlDeviceGetPowerUsage(handle) / 1000.0  # mW to W\n",
                "        except:\n",
                "            power = 0.0\n",
                "            \n",
                "        try:\n",
                "            major, minor = pynvml.nvmlDeviceGetCudaComputeCapability(handle)\n",
                "            cc = (major, minor)\n",
                "        except:\n",
                "            cc = (0, 0)\n",
                "        \n",
                "        return GpuDevice(\n",
                "            device_id=device_id,\n",
                "            name=name,\n",
                "            uuid=uuid,\n",
                "            total_memory=mem_info.total,\n",
                "            free_memory=mem_info.free,\n",
                "            used_memory=mem_info.used,\n",
                "            temperature=temp,\n",
                "            utilization=gpu_util,\n",
                "            power_usage=power,\n",
                "            compute_capability=cc,\n",
                "        )\n",
                "    \n",
                "    def get_all_devices(self) -> List[GpuDevice]:\n",
                "        \"\"\"Get all GPU devices.\"\"\"\n",
                "        return [self.get_device(i) for i in range(self.device_count)]\n",
                "    \n",
                "    def shutdown(self):\n",
                "        pynvml.nvmlShutdown()\n",
                "\n",
                "# Test NVML\n",
                "monitor = NvmlMonitor()\n",
                "devices = monitor.get_all_devices()\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"GPU DEVICES (NVML Equivalent Test)\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "for dev in devices:\n",
                "    print(f\"\\n* GPU {dev.device_id}: {dev.name}\")\n",
                "    print(f\"   UUID: {dev.uuid}\")\n",
                "    print(f\"   Memory: {dev.free_memory / 1e9:.1f} GB free / {dev.total_memory / 1e9:.1f} GB total\")\n",
                "    print(f\"   Temperature: {dev.temperature}°C\")\n",
                "    print(f\"   Utilization: {dev.utilization}%\")\n",
                "    print(f\"   Power: {dev.power_usage:.1f} W\")\n",
                "    print(f\"   Compute Capability: {dev.compute_capability[0]}.{dev.compute_capability[1]}\")\n",
                "\n",
                "print(\"\\n[OK] NVML test passed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section_cuda"
            },
            "source": [
                "##  CUDA Runtime Test\n",
                "\n",
                "Test CUDA memory allocation and operations (similar to `cuda.rs`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_cuda"
            },
            "outputs": [],
            "source": [
                "#@title 5. CUDA Runtime Test (Python equivalent of cuda.rs)\n",
                "import torch\n",
                "import time\n",
                "\n",
                "class CudaRuntime:\n",
                "    \"\"\"\n",
                "    Python equivalent of zenith-runtime-gpu's CUDA wrapper.\n",
                "    Tests the same functionality as cuda.rs.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.available = torch.cuda.is_available()\n",
                "        self.device_count = torch.cuda.device_count() if self.available else 0\n",
                "        self.current_device = 0\n",
                "        \n",
                "        if self.available:\n",
                "            print(f\"[OK] CUDA Runtime initialized\")\n",
                "            print(f\"   CUDA Version: {torch.version.cuda}\")\n",
                "            print(f\"   Device Count: {self.device_count}\")\n",
                "        else:\n",
                "            print(\"[FAIL] CUDA not available\")\n",
                "    \n",
                "    def set_device(self, device_id: int):\n",
                "        \"\"\"Set current CUDA device (cudaSetDevice).\"\"\"\n",
                "        if device_id < self.device_count:\n",
                "            torch.cuda.set_device(device_id)\n",
                "            self.current_device = device_id\n",
                "            return True\n",
                "        return False\n",
                "    \n",
                "    def get_device_properties(self, device_id: int) -> dict:\n",
                "        \"\"\"Get device properties (cudaGetDeviceProperties).\"\"\"\n",
                "        props = torch.cuda.get_device_properties(device_id)\n",
                "        return {\n",
                "            'name': props.name,\n",
                "            'total_memory': props.total_memory,\n",
                "            'major': props.major,\n",
                "            'minor': props.minor,\n",
                "            'multi_processor_count': props.multi_processor_count,\n",
                "        }\n",
                "    \n",
                "    def malloc(self, size: int) -> torch.Tensor:\n",
                "        \"\"\"Allocate device memory (cudaMalloc).\"\"\"\n",
                "        tensor = torch.empty(size, dtype=torch.uint8, device='cuda')\n",
                "        return tensor\n",
                "    \n",
                "    def memcpy_h2d(self, host_data: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"Copy host to device (cudaMemcpy H2D).\"\"\"\n",
                "        return host_data.cuda()\n",
                "    \n",
                "    def memcpy_d2h(self, device_data: torch.Tensor) -> torch.Tensor:\n",
                "        \"\"\"Copy device to host (cudaMemcpy D2H).\"\"\"\n",
                "        return device_data.cpu()\n",
                "    \n",
                "    def synchronize(self):\n",
                "        \"\"\"Synchronize all streams (cudaDeviceSynchronize).\"\"\"\n",
                "        torch.cuda.synchronize()\n",
                "    \n",
                "    def mem_info(self) -> tuple:\n",
                "        \"\"\"Get memory info (cudaMemGetInfo).\"\"\"\n",
                "        free = torch.cuda.mem_get_info()[0]\n",
                "        total = torch.cuda.mem_get_info()[1]\n",
                "        return (free, total)\n",
                "\n",
                "# Test CUDA Runtime\n",
                "cuda = CudaRuntime()\n",
                "\n",
                "if cuda.available:\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"CUDA RUNTIME TEST\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    # Test device properties\n",
                "    for i in range(cuda.device_count):\n",
                "        props = cuda.get_device_properties(i)\n",
                "        print(f\"\\n* Device {i}: {props['name']}\")\n",
                "        print(f\"   Memory: {props['total_memory'] / 1e9:.1f} GB\")\n",
                "        print(f\"   SM Count: {props['multi_processor_count']}\")\n",
                "        print(f\"   Compute: {props['major']}.{props['minor']}\")\n",
                "    \n",
                "    # Test memory allocation\n",
                "    print(\"\\n* Memory Allocation Test:\")\n",
                "    free_before, total = cuda.mem_info()\n",
                "    print(f\"   Before: {free_before / 1e9:.2f} GB free\")\n",
                "    \n",
                "    # Allocate 1GB\n",
                "    alloc_size = 1024 * 1024 * 1024  # 1GB\n",
                "    memory = cuda.malloc(alloc_size)\n",
                "    cuda.synchronize()\n",
                "    \n",
                "    free_after, _ = cuda.mem_info()\n",
                "    print(f\"   After 1GB alloc: {free_after / 1e9:.2f} GB free\")\n",
                "    print(f\"   Allocated: {(free_before - free_after) / 1e9:.2f} GB\")\n",
                "    \n",
                "    # Test memcpy\n",
                "    print(\"\\n* Memory Copy Test:\")\n",
                "    host_data = torch.randn(1000000, dtype=torch.float32)\n",
                "    \n",
                "    start = time.time()\n",
                "    device_data = cuda.memcpy_h2d(host_data)\n",
                "    cuda.synchronize()\n",
                "    h2d_time = time.time() - start\n",
                "    \n",
                "    start = time.time()\n",
                "    result = cuda.memcpy_d2h(device_data)\n",
                "    d2h_time = time.time() - start\n",
                "    \n",
                "    print(f\"   H2D: {h2d_time*1000:.2f} ms ({(host_data.numel()*4) / h2d_time / 1e9:.1f} GB/s)\")\n",
                "    print(f\"   D2H: {d2h_time*1000:.2f} ms ({(host_data.numel()*4) / d2h_time / 1e9:.1f} GB/s)\")\n",
                "    print(f\"   Data integrity: {'[OK] PASS' if torch.allclose(host_data, result) else '[FAIL] FAIL'}\")\n",
                "    \n",
                "    # Cleanup\n",
                "    del memory, device_data, result\n",
                "    torch.cuda.empty_cache()\n",
                "    \n",
                "    print(\"\\n[OK] CUDA Runtime test passed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section_streams"
            },
            "source": [
                "##  CUDA Streams Test\n",
                "\n",
                "Test async operations with CUDA streams"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_streams"
            },
            "outputs": [],
            "source": [
                "#@title 6. CUDA Streams Test\n",
                "import torch\n",
                "import time\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(\"=\"*60)\n",
                "    print(\"CUDA STREAMS TEST\")\n",
                "    print(\"=\"*60)\n",
                "    \n",
                "    # Create streams\n",
                "    stream1 = torch.cuda.Stream()\n",
                "    stream2 = torch.cuda.Stream()\n",
                "    \n",
                "    print(f\"\\n* Created 2 CUDA streams\")\n",
                "    \n",
                "    # Test concurrent execution\n",
                "    size = 10000\n",
                "    a = torch.randn(size, size, device='cuda')\n",
                "    b = torch.randn(size, size, device='cuda')\n",
                "    \n",
                "    # Sequential execution\n",
                "    torch.cuda.synchronize()\n",
                "    start = time.time()\n",
                "    c1 = torch.mm(a, b)\n",
                "    c2 = torch.mm(a, b)\n",
                "    torch.cuda.synchronize()\n",
                "    sequential_time = time.time() - start\n",
                "    \n",
                "    # Concurrent execution with streams\n",
                "    torch.cuda.synchronize()\n",
                "    start = time.time()\n",
                "    with torch.cuda.stream(stream1):\n",
                "        c3 = torch.mm(a, b)\n",
                "    with torch.cuda.stream(stream2):\n",
                "        c4 = torch.mm(a, b)\n",
                "    torch.cuda.synchronize()\n",
                "    concurrent_time = time.time() - start\n",
                "    \n",
                "    print(f\"\\n* Matrix multiplication ({size}x{size}):\")\n",
                "    print(f\"   Sequential: {sequential_time*1000:.2f} ms\")\n",
                "    print(f\"   Concurrent: {concurrent_time*1000:.2f} ms\")\n",
                "    print(f\"   Speedup: {sequential_time/concurrent_time:.2f}x\")\n",
                "    \n",
                "    # Clean up\n",
                "    del a, b, c1, c2, c3, c4\n",
                "    torch.cuda.empty_cache()\n",
                "    \n",
                "    print(\"\\n[OK] CUDA Streams test passed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section_benchmark"
            },
            "source": [
                "##  Performance Benchmark\n",
                "\n",
                "Benchmark data loading performance (core Zenith feature)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_benchmark"
            },
            "outputs": [],
            "source": [
                "#@title 7. Data Loading Benchmark\n",
                "import torch\n",
                "import time\n",
                "import numpy as np\n",
                "\n",
                "def benchmark_data_loading(batch_sizes, num_iterations=100):\n",
                "    \"\"\"\n",
                "    Benchmark simulating Zenith's data loading pipeline.\n",
                "    Measures throughput of CPU→GPU data transfer.\n",
                "    \"\"\"\n",
                "    results = []\n",
                "    \n",
                "    for batch_size in batch_sizes:\n",
                "        # Simulate image batch (3x224x224)\n",
                "        data_shape = (batch_size, 3, 224, 224)\n",
                "        data_size = np.prod(data_shape) * 4  # float32\n",
                "        \n",
                "        # Pre-create data\n",
                "        cpu_data = torch.randn(*data_shape, dtype=torch.float32, pin_memory=True)\n",
                "        \n",
                "        # Warm up\n",
                "        for _ in range(10):\n",
                "            gpu_data = cpu_data.cuda(non_blocking=True)\n",
                "        torch.cuda.synchronize()\n",
                "        \n",
                "        # Benchmark\n",
                "        start = time.time()\n",
                "        for _ in range(num_iterations):\n",
                "            gpu_data = cpu_data.cuda(non_blocking=True)\n",
                "        torch.cuda.synchronize()\n",
                "        elapsed = time.time() - start\n",
                "        \n",
                "        samples_per_sec = (batch_size * num_iterations) / elapsed\n",
                "        throughput_gbps = (data_size * num_iterations) / elapsed / 1e9\n",
                "        latency_ms = elapsed / num_iterations * 1000\n",
                "        \n",
                "        results.append({\n",
                "            'batch_size': batch_size,\n",
                "            'samples_per_sec': samples_per_sec,\n",
                "            'throughput_gbps': throughput_gbps,\n",
                "            'latency_ms': latency_ms,\n",
                "        })\n",
                "        \n",
                "        del cpu_data, gpu_data\n",
                "        torch.cuda.empty_cache()\n",
                "    \n",
                "    return results\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    print(\"=\"*60)\n",
                "    print(\"DATA LOADING BENCHMARK\")\n",
                "    print(\"=\"*60)\n",
                "    print(\"Simulates Zenith's CPU→GPU data loading pipeline\")\n",
                "    print(\"\")\n",
                "    \n",
                "    batch_sizes = [32, 64, 128, 256]\n",
                "    results = benchmark_data_loading(batch_sizes)\n",
                "    \n",
                "    print(f\"{'Batch':<10} {'Samples/s':<15} {'Throughput':<15} {'Latency':<10}\")\n",
                "    print(\"-\" * 50)\n",
                "    \n",
                "    for r in results:\n",
                "        print(f\"{r['batch_size']:<10} {r['samples_per_sec']:<15.0f} {r['throughput_gbps']:<15.2f} GB/s {r['latency_ms']:<10.3f} ms\")\n",
                "    \n",
                "    # Find best throughput\n",
                "    best = max(results, key=lambda x: x['samples_per_sec'])\n",
                "    print(f\"\\n Best: {best['samples_per_sec']:.0f} samples/sec at batch_size={best['batch_size']}\")\n",
                "    \n",
                "    print(\"\\n[OK] Benchmark complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section_multigpu"
            },
            "source": [
                "##  Multi-GPU Test (if available)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "test_multigpu"
            },
            "outputs": [],
            "source": [
                "#@title 8. Multi-GPU Test\n",
                "import torch\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    device_count = torch.cuda.device_count()\n",
                "    \n",
                "    print(\"=\"*60)\n",
                "    print(\"MULTI-GPU TEST\")\n",
                "    print(\"=\"*60)\n",
                "    print(f\"Detected {device_count} GPU(s)\")\n",
                "    \n",
                "    if device_count > 1:\n",
                "        print(\"\\n* Testing peer-to-peer memory access:\")\n",
                "        \n",
                "        for i in range(device_count):\n",
                "            for j in range(device_count):\n",
                "                if i != j:\n",
                "                    can_access = torch.cuda.can_device_access_peer(i, j)\n",
                "                    print(f\"   GPU {i} → GPU {j}: {'[OK] Yes' if can_access else '[FAIL] No'}\")\n",
                "        \n",
                "        print(\"\\n* Testing data transfer between GPUs:\")\n",
                "        size = 1000000\n",
                "        \n",
                "        for i in range(device_count):\n",
                "            for j in range(device_count):\n",
                "                if i != j:\n",
                "                    # Create tensor on GPU i\n",
                "                    with torch.cuda.device(i):\n",
                "                        src = torch.randn(size, device=f'cuda:{i}')\n",
                "                    \n",
                "                    # Copy to GPU j\n",
                "                    torch.cuda.synchronize()\n",
                "                    start = time.time()\n",
                "                    dst = src.to(f'cuda:{j}')\n",
                "                    torch.cuda.synchronize()\n",
                "                    elapsed = time.time() - start\n",
                "                    \n",
                "                    bandwidth = (size * 4) / elapsed / 1e9\n",
                "                    print(f\"   GPU {i} → GPU {j}: {bandwidth:.1f} GB/s\")\n",
                "                    \n",
                "                    del src, dst\n",
                "        \n",
                "        torch.cuda.empty_cache()\n",
                "        print(\"\\n[OK] Multi-GPU test passed!\")\n",
                "    else:\n",
                "        print(\"\\n[!] Only 1 GPU available. Multi-GPU test skipped.\")\n",
                "        print(\"   (Upgrade to Colab Pro for multi-GPU access)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "section_summary"
            },
            "source": [
                "##  Test Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "summary"
            },
            "outputs": [],
            "source": [
                "#@title 9. Generate Test Report\n",
                "import torch\n",
                "import pynvml\n",
                "from datetime import datetime\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"ZENITH GPU RUNTIME - TEST REPORT\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Date: {datetime.now().isoformat()}\")\n",
                "print(\"\")\n",
                "\n",
                "# Environment\n",
                "print(\" ENVIRONMENT\")\n",
                "print(f\"   PyTorch: {torch.__version__}\")\n",
                "print(f\"   CUDA: {torch.version.cuda}\")\n",
                "print(f\"   cuDNN: {torch.backends.cudnn.version()}\")\n",
                "print(\"\")\n",
                "\n",
                "# GPU Info\n",
                "if torch.cuda.is_available():\n",
                "    print(\" GPU HARDWARE\")\n",
                "    for i in range(torch.cuda.device_count()):\n",
                "        props = torch.cuda.get_device_properties(i)\n",
                "        pynvml.nvmlInit()\n",
                "        handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
                "        mem = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
                "        print(f\"   GPU {i}: {props.name}\")\n",
                "        print(f\"      Memory: {mem.free/1e9:.1f} / {mem.total/1e9:.1f} GB\")\n",
                "        print(f\"      Compute: {props.major}.{props.minor}\")\n",
                "        pynvml.nvmlShutdown()\n",
                "    print(\"\")\n",
                "\n",
                "# Test Results\n",
                "print(\" TEST RESULTS\")\n",
                "tests = [\n",
                "    (\"NVML Detection\", True),\n",
                "    (\"CUDA Runtime\", torch.cuda.is_available()),\n",
                "    (\"Device Properties\", torch.cuda.is_available()),\n",
                "    (\"Memory Allocation\", torch.cuda.is_available()),\n",
                "    (\"Memory Copy H2D\", torch.cuda.is_available()),\n",
                "    (\"Memory Copy D2H\", torch.cuda.is_available()),\n",
                "    (\"CUDA Streams\", torch.cuda.is_available()),\n",
                "    (\"Data Loading\", torch.cuda.is_available()),\n",
                "    (\"Multi-GPU\", torch.cuda.device_count() > 1),\n",
                "]\n",
                "\n",
                "passed = sum(1 for _, status in tests if status)\n",
                "total = len(tests)\n",
                "\n",
                "for test, status in tests:\n",
                "    icon = \"[OK]\" if status else \"[!]\"\n",
                "    print(f\"   {icon} {test}\")\n",
                "\n",
                "print(\"\")\n",
                "print(f\" SUMMARY: {passed}/{total} tests passed\")\n",
                "print(\"\")\n",
                "\n",
                "if passed == total:\n",
                "    print(\" All GPU tests PASSED!\")\n",
                "    print(\"   zenith-runtime-gpu is compatible with this environment.\")\n",
                "else:\n",
                "    print(\"[!] Some tests skipped (may need multi-GPU or Colab Pro)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "next_steps"
            },
            "source": [
                "##  Next Steps\n",
                "\n",
                "This notebook validates that:\n",
                "\n",
                "1. **NVML** - GPU detection works (tests `nvml.rs` logic)\n",
                "2. **CUDA Runtime** - Memory allocation/copy works (tests `cuda.rs` logic)\n",
                "3. **Streams** - Async operations work\n",
                "4. **Data Loading** - CPU→GPU transfer is fast\n",
                "\n",
                "To compile actual Rust code on Colab:\n",
                "\n",
                "```bash\n",
                "# Install Rust\n",
                "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\n",
                "!source ~/.cargo/env\n",
                "\n",
                "# Build zenith-runtime-gpu\n",
                "!cd Zenith-dataplane && cargo build -p zenith-runtime-gpu --release\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}