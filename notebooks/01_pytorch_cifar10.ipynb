{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith AI + PyTorch: Image Classification\n",
                "\n",
                "**Train a CNN on CIFAR-10 with Zenith's high-performance DataLoader**\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vibeswithkk/Zenith-dataplane/blob/main/notebooks/01_pytorch_cifar10.ipynb)\n",
                "\n",
                "## What You'll Learn\n",
                "- Install and use Zenith in 2 lines\n",
                "- Load CIFAR-10 with Zenith DataLoader\n",
                "- Train a CNN with zero-copy data transfer\n",
                "- Compare speed vs PyTorch native DataLoader"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Install Zenith"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install zenith-ai (takes ~10 seconds)\n",
                "!pip install zenith-ai torch torchvision datasets pyarrow --quiet\n",
                "\n",
                "# Verify installation\n",
                "import zenith\n",
                "zenith.info()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Download CIFAR-10 Dataset (~170MB)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "import pyarrow.parquet as pq\n",
                "import pyarrow as pa\n",
                "import numpy as np\n",
                "\n",
                "print(\"Downloading CIFAR-10...\")\n",
                "dataset = load_dataset(\"cifar10\", split=\"train\")\n",
                "print(f\"Dataset size: {len(dataset)} images\")\n",
                "\n",
                "# Convert to Parquet for Zenith\n",
                "print(\"\\nConverting to Parquet format...\")\n",
                "\n",
                "# Extract images and labels\n",
                "images = [np.array(img['img']).flatten().tobytes() for img in dataset]\n",
                "labels = [img['label'] for img in dataset]\n",
                "\n",
                "# Create Arrow table\n",
                "table = pa.table({\n",
                "    'image': images,\n",
                "    'label': labels\n",
                "})\n",
                "\n",
                "# Save as Parquet\n",
                "pq.write_table(table, 'cifar10_train.parquet')\n",
                "print(f\"Saved: cifar10_train.parquet\")\n",
                "\n",
                "import os\n",
                "size_mb = os.path.getsize('cifar10_train.parquet') / (1024 * 1024)\n",
                "print(f\"File size: {size_mb:.1f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Define CNN Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class SimpleCNN(nn.Module):\n",
                "    \"\"\"Simple CNN for CIFAR-10 classification.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
                "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
                "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
                "        self.pool = nn.MaxPool2d(2, 2)\n",
                "        self.fc1 = nn.Linear(64 * 4 * 4, 512)\n",
                "        self.fc2 = nn.Linear(512, 10)\n",
                "        self.dropout = nn.Dropout(0.25)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.pool(F.relu(self.conv1(x)))  # 32x32 -> 16x16\n",
                "        x = self.pool(F.relu(self.conv2(x)))  # 16x16 -> 8x8\n",
                "        x = self.pool(F.relu(self.conv3(x)))  # 8x8 -> 4x4\n",
                "        x = x.view(-1, 64 * 4 * 4)\n",
                "        x = self.dropout(F.relu(self.fc1(x)))\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "# Initialize model\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "model = SimpleCNN().to(device)\n",
                "print(f\"Model on: {device}\")\n",
                "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Load Data with Zenith DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import zenith\n",
                "import time\n",
                "\n",
                "# Create Zenith DataLoader\n",
                "loader = zenith.DataLoader(\n",
                "    'cifar10_train.parquet',\n",
                "    batch_size=64,\n",
                "    shuffle=True,\n",
                "    device='auto'  # Auto-detect GPU\n",
                ")\n",
                "\n",
                "print(f\"DataLoader created: {loader}\")\n",
                "print(f\"Device: {zenith.auto_device()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Training Loop with Zenith"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.optim as optim\n",
                "\n",
                "# Setup training\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "def train_epoch(loader, model, criterion, optimizer, device):\n",
                "    \"\"\"Train one epoch with Zenith DataLoader.\"\"\"\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for batch in loader:\n",
                "        # Zero-copy conversion to PyTorch tensors\n",
                "        data = batch.to_numpy()\n",
                "        \n",
                "        # Reshape images: bytes -> (B, 3, 32, 32)\n",
                "        images = np.array([np.frombuffer(img, dtype=np.uint8).reshape(32, 32, 3) \n",
                "                          for img in data['image']])\n",
                "        images = torch.from_numpy(images).permute(0, 3, 1, 2).float() / 255.0\n",
                "        labels = torch.from_numpy(data['label']).long()\n",
                "        \n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        # Forward pass\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        \n",
                "        # Backward pass\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        # Stats\n",
                "        total_loss += loss.item()\n",
                "        _, predicted = outputs.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += predicted.eq(labels).sum().item()\n",
                "    \n",
                "    return total_loss / total, 100. * correct / total\n",
                "\n",
                "# Train for 3 epochs\n",
                "print(\"Training with Zenith DataLoader...\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "zenith_times = []\n",
                "for epoch in range(3):\n",
                "    start = time.time()\n",
                "    loss, acc = train_epoch(loader, model, criterion, optimizer, device)\n",
                "    elapsed = time.time() - start\n",
                "    zenith_times.append(elapsed)\n",
                "    print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Acc={acc:.2f}%, Time={elapsed:.2f}s\")\n",
                "\n",
                "print(\"-\" * 40)\n",
                "print(f\"Average time per epoch: {sum(zenith_times)/len(zenith_times):.2f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Benchmark: Zenith vs PyTorch DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import DataLoader as TorchDataLoader, TensorDataset\n",
                "import time\n",
                "\n",
                "# Prepare PyTorch DataLoader with same data\n",
                "print(\"Preparing PyTorch DataLoader...\")\n",
                "\n",
                "# Load data into memory for PyTorch\n",
                "dataset_hf = load_dataset(\"cifar10\", split=\"train[:5000]\")  # Subset for fair comparison\n",
                "images_pt = torch.stack([torch.from_numpy(np.array(x['img'])).permute(2,0,1).float()/255.0 \n",
                "                         for x in dataset_hf])\n",
                "labels_pt = torch.tensor([x['label'] for x in dataset_hf])\n",
                "\n",
                "torch_loader = TorchDataLoader(\n",
                "    TensorDataset(images_pt, labels_pt),\n",
                "    batch_size=64,\n",
                "    shuffle=True\n",
                ")\n",
                "\n",
                "# Benchmark PyTorch\n",
                "print(\"\\nBenchmarking data loading speed...\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# PyTorch DataLoader\n",
                "start = time.time()\n",
                "for _ in range(3):\n",
                "    for batch in torch_loader:\n",
                "        _ = batch[0].to(device)\n",
                "pytorch_time = time.time() - start\n",
                "\n",
                "# Zenith DataLoader\n",
                "zenith_loader = zenith.DataLoader('cifar10_train.parquet', batch_size=64)\n",
                "start = time.time()\n",
                "for _ in range(3):\n",
                "    for batch in zenith_loader:\n",
                "        _ = batch.to_numpy()\n",
                "zenith_time = time.time() - start\n",
                "\n",
                "print(f\"PyTorch DataLoader: {pytorch_time:.3f}s\")\n",
                "print(f\"Zenith DataLoader:  {zenith_time:.3f}s\")\n",
                "speedup = pytorch_time/zenith_time\n",
                "print(f\"\\nZenith is {speedup:.1f}x faster\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "You've learned:\n",
                "1. Install Zenith with `pip install zenith-ai`\n",
                "2. Load data with `zenith.DataLoader()`\n",
                "3. Zero-copy conversion with `batch.to_numpy()`\n",
                "4. Train PyTorch models faster\n",
                "\n",
                "### Next Steps\n",
                "- Try with your own datasets\n",
                "- Use GPU: `device=\"cuda\"`\n",
                "- Scale to larger datasets (S3, etc.)\n",
                "\n",
                "**GitHub:** https://github.com/vibeswithkk/Zenith-dataplane"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}