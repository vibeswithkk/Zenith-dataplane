{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith AI + PyTorch: Image Classification\n",
                "\n",
                "**Train a CNN on CIFAR-10 with Zenith's high-performance DataLoader**\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vibeswithkk/Zenith-dataplane/blob/main/notebooks/01_pytorch_cifar10.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install pyarrow datasets torch torchvision --quiet\n",
                "print(\"Dependencies installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Download CIFAR-10 Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "import pyarrow.parquet as pq\n",
                "import pyarrow as pa\n",
                "import numpy as np\n",
                "\n",
                "print(\"Downloading CIFAR-10 (subset for demo)...\")\n",
                "dataset = load_dataset(\"cifar10\", split=\"train[:10000]\")\n",
                "print(f\"Dataset size: {len(dataset)} images\")\n",
                "\n",
                "# Convert to Parquet\n",
                "print(\"Converting to Parquet...\")\n",
                "\n",
                "# Store as float arrays directly\n",
                "images = np.array([np.array(x['img'], dtype=np.float32).transpose(2,0,1).flatten() / 255.0 \n",
                "                   for x in dataset])\n",
                "labels = np.array([x['label'] for x in dataset])\n",
                "\n",
                "# Save as Parquet with proper column types\n",
                "table = pa.table({\n",
                "    'image': pa.array([img.tobytes() for img in images]),\n",
                "    'label': pa.array(labels, type=pa.int64())\n",
                "})\n",
                "pq.write_table(table, 'cifar10_train.parquet')\n",
                "\n",
                "import os\n",
                "size_mb = os.path.getsize('cifar10_train.parquet') / (1024 * 1024)\n",
                "print(f\"Saved: cifar10_train.parquet ({size_mb:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Define CNN Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class SimpleCNN(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
                "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
                "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
                "        self.pool = nn.MaxPool2d(2, 2)\n",
                "        self.fc1 = nn.Linear(64 * 4 * 4, 512)\n",
                "        self.fc2 = nn.Linear(512, 10)\n",
                "        self.dropout = nn.Dropout(0.25)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.pool(F.relu(self.conv1(x)))\n",
                "        x = self.pool(F.relu(self.conv2(x)))\n",
                "        x = self.pool(F.relu(self.conv3(x)))\n",
                "        x = x.reshape(-1, 64 * 4 * 4)  # Use reshape instead of view\n",
                "        x = self.dropout(F.relu(self.fc1(x)))\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "model = SimpleCNN().to(device)\n",
                "print(f\"Device: {device}\")\n",
                "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Zenith DataLoader (Pure Python)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pyarrow.parquet as pq\n",
                "import numpy as np\n",
                "\n",
                "class ZenithDataLoader:\n",
                "    \"\"\"High-performance DataLoader using Arrow/Parquet.\"\"\"\n",
                "    \n",
                "    def __init__(self, path, batch_size=64, shuffle=True):\n",
                "        self.table = pq.read_table(path)\n",
                "        self.batch_size = batch_size\n",
                "        self.shuffle = shuffle\n",
                "        self.num_rows = self.table.num_rows\n",
                "    \n",
                "    def __iter__(self):\n",
                "        indices = np.arange(self.num_rows)\n",
                "        if self.shuffle:\n",
                "            np.random.shuffle(indices)\n",
                "        \n",
                "        for start in range(0, self.num_rows, self.batch_size):\n",
                "            end = min(start + self.batch_size, self.num_rows)\n",
                "            batch_idx = indices[start:end]\n",
                "            batch = self.table.take(batch_idx)\n",
                "            yield batch\n",
                "    \n",
                "    def __len__(self):\n",
                "        return (self.num_rows + self.batch_size - 1) // self.batch_size\n",
                "\n",
                "loader = ZenithDataLoader('cifar10_train.parquet', batch_size=64)\n",
                "print(f\"DataLoader: {len(loader)} batches\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.optim as optim\n",
                "import time\n",
                "\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "def train_epoch(loader, model, criterion, optimizer, device):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for batch in loader:\n",
                "        # Convert Arrow to numpy\n",
                "        img_bytes = batch.column('image').to_pylist()\n",
                "        labels_np = batch.column('label').to_numpy()\n",
                "        \n",
                "        # Reconstruct images from bytes\n",
                "        images_np = np.array([np.frombuffer(b, dtype=np.float32).reshape(3, 32, 32) \n",
                "                              for b in img_bytes])\n",
                "        \n",
                "        images = torch.from_numpy(images_np).to(device)\n",
                "        labels = torch.from_numpy(labels_np).long().to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        _, predicted = outputs.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += predicted.eq(labels).sum().item()\n",
                "    \n",
                "    return total_loss / len(loader), 100. * correct / total\n",
                "\n",
                "print(\"Training with Zenith DataLoader...\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "zenith_times = []\n",
                "for epoch in range(3):\n",
                "    start = time.time()\n",
                "    loss, acc = train_epoch(loader, model, criterion, optimizer, device)\n",
                "    elapsed = time.time() - start\n",
                "    zenith_times.append(elapsed)\n",
                "    print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Acc={acc:.2f}%, Time={elapsed:.2f}s\")\n",
                "\n",
                "zenith_avg = sum(zenith_times) / len(zenith_times)\n",
                "print(\"-\" * 40)\n",
                "print(f\"Zenith avg time: {zenith_avg:.2f}s per epoch\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Benchmark vs PyTorch DataLoader"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import DataLoader as TorchDataLoader, TensorDataset\n",
                "\n",
                "print(\"Loading data for PyTorch DataLoader...\")\n",
                "\n",
                "# Load same data for PyTorch\n",
                "table = pq.read_table('cifar10_train.parquet')\n",
                "img_bytes = table.column('image').to_pylist()\n",
                "labels_np = table.column('label').to_numpy()\n",
                "\n",
                "images_np = np.array([np.frombuffer(b, dtype=np.float32).reshape(3, 32, 32) \n",
                "                      for b in img_bytes])\n",
                "\n",
                "images_tensor = torch.from_numpy(images_np)\n",
                "labels_tensor = torch.from_numpy(labels_np).long()\n",
                "\n",
                "torch_loader = TorchDataLoader(\n",
                "    TensorDataset(images_tensor, labels_tensor),\n",
                "    batch_size=64,\n",
                "    shuffle=True\n",
                ")\n",
                "\n",
                "# Reset model\n",
                "model = SimpleCNN().to(device)\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "def train_epoch_torch(loader, model, criterion, optimizer, device):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    for images, labels in loader:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "        _, predicted = outputs.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += predicted.eq(labels).sum().item()\n",
                "    \n",
                "    return total_loss / len(loader), 100. * correct / total\n",
                "\n",
                "print(\"\\nTraining with PyTorch DataLoader...\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "torch_times = []\n",
                "for epoch in range(3):\n",
                "    start = time.time()\n",
                "    loss, acc = train_epoch_torch(torch_loader, model, criterion, optimizer, device)\n",
                "    elapsed = time.time() - start\n",
                "    torch_times.append(elapsed)\n",
                "    print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Acc={acc:.2f}%, Time={elapsed:.2f}s\")\n",
                "\n",
                "torch_avg = sum(torch_times) / len(torch_times)\n",
                "print(\"-\" * 40)\n",
                "print(f\"PyTorch avg time: {torch_avg:.2f}s per epoch\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Results Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*50)\n",
                "print(\"BENCHMARK RESULTS\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Zenith DataLoader:  {zenith_avg:.2f}s per epoch\")\n",
                "print(f\"PyTorch DataLoader: {torch_avg:.2f}s per epoch\")\n",
                "print()\n",
                "\n",
                "if zenith_avg < torch_avg:\n",
                "    speedup = torch_avg / zenith_avg\n",
                "    print(f\"Zenith is {speedup:.2f}x faster\")\n",
                "else:\n",
                "    slowdown = zenith_avg / torch_avg\n",
                "    print(f\"PyTorch is {slowdown:.2f}x faster\")\n",
                "    print(\"(Note: For small datasets, overhead may dominate)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "This notebook demonstrated:\n",
                "1. Loading CIFAR-10 data with Arrow/Parquet\n",
                "2. Training a CNN with Zenith DataLoader\n",
                "3. Fair benchmark comparison\n",
                "\n",
                "**GitHub:** https://github.com/vibeswithkk/Zenith-dataplane"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}