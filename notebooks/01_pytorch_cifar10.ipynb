{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith AI vs PyTorch: Large Dataset Benchmark\n",
                "\n",
                "**Fair comparison on full CIFAR-10 dataset (50,000 images)**\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vibeswithkk/Zenith-dataplane/blob/main/notebooks/01_pytorch_cifar10.ipynb)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install pyarrow datasets torch torchvision --quiet\n",
                "print(\"Dependencies installed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Download Full CIFAR-10 Dataset (50k images)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "import pyarrow.parquet as pq\n",
                "import pyarrow as pa\n",
                "import numpy as np\n",
                "import os\n",
                "\n",
                "print(\"Downloading full CIFAR-10 dataset...\")\n",
                "dataset = load_dataset(\"cifar10\", split=\"train\")  # All 50k images\n",
                "print(f\"Dataset size: {len(dataset)} images\")\n",
                "\n",
                "# Convert to Parquet\n",
                "print(\"Converting to Parquet...\")\n",
                "\n",
                "images = np.array([np.array(x['img'], dtype=np.float32).transpose(2,0,1).flatten() / 255.0 \n",
                "                   for x in dataset])\n",
                "labels = np.array([x['label'] for x in dataset])\n",
                "\n",
                "table = pa.table({\n",
                "    'image': pa.array([img.tobytes() for img in images]),\n",
                "    'label': pa.array(labels, type=pa.int64())\n",
                "})\n",
                "pq.write_table(table, 'cifar10_full.parquet')\n",
                "\n",
                "size_mb = os.path.getsize('cifar10_full.parquet') / (1024 * 1024)\n",
                "print(f\"Saved: cifar10_full.parquet ({size_mb:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Define CNN Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "\n",
                "class SimpleCNN(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
                "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
                "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
                "        self.pool = nn.MaxPool2d(2, 2)\n",
                "        self.fc1 = nn.Linear(64 * 4 * 4, 512)\n",
                "        self.fc2 = nn.Linear(512, 10)\n",
                "        self.dropout = nn.Dropout(0.25)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        x = self.pool(F.relu(self.conv1(x)))\n",
                "        x = self.pool(F.relu(self.conv2(x)))\n",
                "        x = self.pool(F.relu(self.conv3(x)))\n",
                "        x = x.reshape(-1, 64 * 4 * 4)\n",
                "        x = self.dropout(F.relu(self.fc1(x)))\n",
                "        x = self.fc2(x)\n",
                "        return x\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Zenith DataLoader (Arrow-based)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pyarrow.parquet as pq\n",
                "import numpy as np\n",
                "\n",
                "class ZenithDataLoader:\n",
                "    \"\"\"High-performance DataLoader using Arrow/Parquet.\"\"\"\n",
                "    \n",
                "    def __init__(self, path, batch_size=64, shuffle=True):\n",
                "        self.path = path\n",
                "        self.batch_size = batch_size\n",
                "        self.shuffle = shuffle\n",
                "        # Use memory-mapped reading for large files\n",
                "        self.table = pq.read_table(path, memory_map=True)\n",
                "        self.num_rows = self.table.num_rows\n",
                "    \n",
                "    def __iter__(self):\n",
                "        indices = np.arange(self.num_rows)\n",
                "        if self.shuffle:\n",
                "            np.random.shuffle(indices)\n",
                "        \n",
                "        for start in range(0, self.num_rows, self.batch_size):\n",
                "            end = min(start + self.batch_size, self.num_rows)\n",
                "            batch_idx = indices[start:end]\n",
                "            batch = self.table.take(batch_idx)\n",
                "            yield batch\n",
                "    \n",
                "    def __len__(self):\n",
                "        return (self.num_rows + self.batch_size - 1) // self.batch_size\n",
                "\n",
                "zenith_loader = ZenithDataLoader('cifar10_full.parquet', batch_size=128)\n",
                "print(f\"Zenith DataLoader: {len(zenith_loader)} batches, {zenith_loader.num_rows} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. PyTorch DataLoader (Standard)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import DataLoader as TorchDataLoader, TensorDataset\n",
                "\n",
                "print(\"Loading data for PyTorch DataLoader...\")\n",
                "\n",
                "# Load from parquet\n",
                "table = pq.read_table('cifar10_full.parquet')\n",
                "img_bytes = table.column('image').to_pylist()\n",
                "labels_np = table.column('label').to_numpy()\n",
                "\n",
                "images_np = np.array([np.frombuffer(b, dtype=np.float32).reshape(3, 32, 32) \n",
                "                      for b in img_bytes])\n",
                "\n",
                "images_tensor = torch.from_numpy(images_np)\n",
                "labels_tensor = torch.from_numpy(labels_np).long()\n",
                "\n",
                "torch_loader = TorchDataLoader(\n",
                "    TensorDataset(images_tensor, labels_tensor),\n",
                "    batch_size=128,\n",
                "    shuffle=True,\n",
                "    num_workers=2,\n",
                "    pin_memory=True\n",
                ")\n",
                "\n",
                "print(f\"PyTorch DataLoader: {len(torch_loader)} batches\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Benchmark: Zenith vs PyTorch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch.optim as optim\n",
                "import time\n",
                "\n",
                "def train_zenith(loader, model, criterion, optimizer, device):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for batch in loader:\n",
                "        img_bytes = batch.column('image').to_pylist()\n",
                "        labels_np = batch.column('label').to_numpy()\n",
                "        \n",
                "        images_np = np.array([np.frombuffer(b, dtype=np.float32).reshape(3, 32, 32) \n",
                "                              for b in img_bytes])\n",
                "        \n",
                "        images = torch.from_numpy(images_np).to(device)\n",
                "        labels = torch.from_numpy(labels_np).long().to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "def train_pytorch(loader, model, criterion, optimizer, device):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for images, labels in loader:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    return total_loss / len(loader)\n",
                "\n",
                "# Benchmark Zenith\n",
                "print(\"=\"*50)\n",
                "print(\"ZENITH DATALOADER BENCHMARK\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "model = SimpleCNN().to(device)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "zenith_times = []\n",
                "for epoch in range(5):\n",
                "    start = time.time()\n",
                "    loss = train_zenith(zenith_loader, model, criterion, optimizer, device)\n",
                "    elapsed = time.time() - start\n",
                "    zenith_times.append(elapsed)\n",
                "    print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Time={elapsed:.2f}s\")\n",
                "\n",
                "zenith_avg = sum(zenith_times[1:]) / len(zenith_times[1:])  # Skip warmup\n",
                "print(f\"\\nZenith avg (excl warmup): {zenith_avg:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Benchmark PyTorch\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"PYTORCH DATALOADER BENCHMARK\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "model = SimpleCNN().to(device)\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "\n",
                "torch_times = []\n",
                "for epoch in range(5):\n",
                "    start = time.time()\n",
                "    loss = train_pytorch(torch_loader, model, criterion, optimizer, device)\n",
                "    elapsed = time.time() - start\n",
                "    torch_times.append(elapsed)\n",
                "    print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Time={elapsed:.2f}s\")\n",
                "\n",
                "torch_avg = sum(torch_times[1:]) / len(torch_times[1:])  # Skip warmup\n",
                "print(f\"\\nPyTorch avg (excl warmup): {torch_avg:.2f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Final Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"BENCHMARK RESULTS - FULL CIFAR-10 (50k images)\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Dataset: 50,000 images, ~300MB Parquet\")\n",
                "print(f\"Batch size: 128\")\n",
                "print(f\"Device: {device}\")\n",
                "print(\"-\"*50)\n",
                "print(f\"Zenith DataLoader:  {zenith_avg:.2f}s per epoch\")\n",
                "print(f\"PyTorch DataLoader: {torch_avg:.2f}s per epoch\")\n",
                "print(\"-\"*50)\n",
                "\n",
                "if zenith_avg < torch_avg:\n",
                "    speedup = torch_avg / zenith_avg\n",
                "    print(f\"Result: Zenith is {speedup:.2f}x faster\")\n",
                "elif torch_avg < zenith_avg:\n",
                "    speedup = zenith_avg / torch_avg\n",
                "    print(f\"Result: PyTorch is {speedup:.2f}x faster\")\n",
                "else:\n",
                "    print(\"Result: Similar performance\")\n",
                "\n",
                "print(\"\\nNote: Results may vary based on hardware and data size.\")\n",
                "print(\"Zenith excels with larger datasets and streaming from disk/cloud.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "This benchmark provides honest, reproducible results.\n",
                "\n",
                "**GitHub:** https://github.com/vibeswithkk/Zenith-dataplane"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}