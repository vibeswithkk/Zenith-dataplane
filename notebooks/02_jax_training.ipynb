{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith AI + JAX: Neural Network Training\n",
                "\n",
                "**Train a neural network with JAX using Zenith's high-performance DataLoader**\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vibeswithkk/Zenith-dataplane/blob/main/notebooks/02_jax_training.ipynb)\n",
                "\n",
                "## What You'll Learn\n",
                "- Load data with Zenith for JAX training\n",
                "- Convert Arrow batches to JAX arrays\n",
                "- Train a simple MLP with high-performance data loading"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install zenith-ai and JAX\n",
                "!pip install zenith-ai jax jaxlib flax optax datasets pyarrow --quiet\n",
                "\n",
                "# Verify installation\n",
                "import zenith\n",
                "import jax\n",
                "zenith.info()\n",
                "print(f\"JAX devices: {jax.devices()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Prepare Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "import pyarrow.parquet as pq\n",
                "import pyarrow as pa\n",
                "import numpy as np\n",
                "\n",
                "print(\"Downloading MNIST dataset...\")\n",
                "dataset = load_dataset(\"mnist\", split=\"train\")\n",
                "print(f\"Dataset size: {len(dataset)} samples\")\n",
                "\n",
                "# Convert to Parquet\n",
                "print(\"\\nConverting to Parquet format...\")\n",
                "\n",
                "images = [np.array(x['image']).flatten().astype(np.float32) / 255.0 for x in dataset]\n",
                "labels = [x['label'] for x in dataset]\n",
                "\n",
                "# Create Arrow table with flattened images\n",
                "table = pa.table({\n",
                "    'pixels': [img.tobytes() for img in images],\n",
                "    'label': labels\n",
                "})\n",
                "\n",
                "pq.write_table(table, 'mnist_train.parquet')\n",
                "\n",
                "import os\n",
                "size_mb = os.path.getsize('mnist_train.parquet') / (1024 * 1024)\n",
                "print(f\"Saved: mnist_train.parquet ({size_mb:.1f} MB)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Define JAX/Flax Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import jax.numpy as jnp\n",
                "from flax import linen as nn\n",
                "from flax.training import train_state\n",
                "import optax\n",
                "\n",
                "class MLP(nn.Module):\n",
                "    \"\"\"Simple MLP for MNIST classification.\"\"\"\n",
                "    \n",
                "    @nn.compact\n",
                "    def __call__(self, x, training: bool = True):\n",
                "        x = nn.Dense(256)(x)\n",
                "        x = nn.relu(x)\n",
                "        x = nn.Dropout(0.2, deterministic=not training)(x)\n",
                "        x = nn.Dense(128)(x)\n",
                "        x = nn.relu(x)\n",
                "        x = nn.Dense(10)(x)\n",
                "        return x\n",
                "\n",
                "# Initialize model\n",
                "model = MLP()\n",
                "rng = jax.random.PRNGKey(0)\n",
                "params = model.init(rng, jnp.ones([1, 784]))\n",
                "\n",
                "# Create optimizer\n",
                "tx = optax.adam(learning_rate=0.001)\n",
                "state = train_state.TrainState.create(\n",
                "    apply_fn=model.apply,\n",
                "    params=params,\n",
                "    tx=tx\n",
                ")\n",
                "\n",
                "print(f\"Model initialized\")\n",
                "print(f\"Parameters: {sum(x.size for x in jax.tree_leaves(params)):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Load Data with Zenith"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import zenith\n",
                "\n",
                "# Create Zenith DataLoader\n",
                "loader = zenith.DataLoader(\n",
                "    'mnist_train.parquet',\n",
                "    batch_size=128,\n",
                "    shuffle=True\n",
                ")\n",
                "\n",
                "print(f\"DataLoader: {loader}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "@jax.jit\n",
                "def train_step(state, images, labels):\n",
                "    \"\"\"Single training step.\"\"\"\n",
                "    def loss_fn(params):\n",
                "        logits = state.apply_fn(params, images, training=True)\n",
                "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, labels).mean()\n",
                "        return loss, logits\n",
                "    \n",
                "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
                "    (loss, logits), grads = grad_fn(state.params)\n",
                "    state = state.apply_gradients(grads=grads)\n",
                "    \n",
                "    accuracy = (logits.argmax(-1) == labels).mean()\n",
                "    return state, loss, accuracy\n",
                "\n",
                "# Training loop\n",
                "print(\"Training with Zenith DataLoader...\")\n",
                "print(\"-\" * 40)\n",
                "\n",
                "for epoch in range(3):\n",
                "    start = time.time()\n",
                "    total_loss = 0\n",
                "    total_acc = 0\n",
                "    num_batches = 0\n",
                "    \n",
                "    for batch in loader:\n",
                "        # Convert Zenith batch to JAX arrays\n",
                "        data = batch.to_numpy()\n",
                "        \n",
                "        # Reconstruct images from bytes\n",
                "        images = jnp.array([np.frombuffer(p, dtype=np.float32) for p in data['pixels']])\n",
                "        labels = jnp.array(data['label'])\n",
                "        \n",
                "        # Train step\n",
                "        state, loss, acc = train_step(state, images, labels)\n",
                "        total_loss += float(loss)\n",
                "        total_acc += float(acc)\n",
                "        num_batches += 1\n",
                "    \n",
                "    elapsed = time.time() - start\n",
                "    print(f\"Epoch {epoch+1}: Loss={total_loss/num_batches:.4f}, \"\n",
                "          f\"Acc={100*total_acc/num_batches:.2f}%, Time={elapsed:.2f}s\")\n",
                "\n",
                "print(\"-\" * 40)\n",
                "print(\"Training complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Summary\n",
                "\n",
                "You've learned:\n",
                "1. Load data with `zenith.DataLoader()`\n",
                "2. Convert to JAX arrays with `batch.to_numpy()` then `jnp.array()`\n",
                "3. Train JAX/Flax models with Zenith data loading\n",
                "\n",
                "**GitHub:** https://github.com/vibeswithkk/Zenith-dataplane"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}