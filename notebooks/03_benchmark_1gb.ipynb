{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Zenith vs PyTorch: 1GB Dataset Benchmark\n",
                "\n",
                "Official benchmark using optimized zenith-ai package v0.3.2+"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install zenith-ai torch pyarrow --quiet\n",
                "import zenith\n",
                "zenith.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pyarrow as pa\n",
                "import pyarrow.parquet as pq\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader as TorchDataLoader, TensorDataset\n",
                "import time\n",
                "import os\n",
                "\n",
                "# Config\n",
                "NUM_SAMPLES = 100000\n",
                "BATCH_SIZE = 256\n",
                "EPOCHS = 3\n",
                "device = zenith.auto_device()\n",
                "print(f\"Device: {device}\")\n",
                "print(f\"Samples: {NUM_SAMPLES:,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate 1GB synthetic data\n",
                "# Store as flattened float32 arrays (consistent format)\n",
                "print(\"Generating synthetic data...\")\n",
                "data_images = np.random.rand(NUM_SAMPLES, 3, 32, 32).astype(np.float32)\n",
                "data_labels = np.random.randint(0, 10, NUM_SAMPLES).astype(np.int64)\n",
                "print(f\"Shape: {data_images.shape}, Size: {data_images.nbytes/1e9:.2f} GB\")\n",
                "\n",
                "# Save as Parquet with proper numeric columns\n",
                "print(\"Saving Parquet...\")\n",
                "table = pa.table({\n",
                "    'features': pa.array(data_images.reshape(NUM_SAMPLES, -1).tolist()),\n",
                "    'label': data_labels\n",
                "})\n",
                "pq.write_table(table, 'data_1gb.parquet')\n",
                "print(f\"Saved: {os.path.getsize('data_1gb.parquet')/1e6:.0f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model\n",
                "class CNN(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.c1 = nn.Conv2d(3, 32, 3, padding=1)\n",
                "        self.c2 = nn.Conv2d(32, 64, 3, padding=1)\n",
                "        self.pool = nn.MaxPool2d(2)\n",
                "        self.fc1 = nn.Linear(64*8*8, 256)\n",
                "        self.fc2 = nn.Linear(256, 10)\n",
                "    def forward(self, x):\n",
                "        x = self.pool(F.relu(self.c1(x)))\n",
                "        x = self.pool(F.relu(self.c2(x)))\n",
                "        x = x.reshape(-1, 64*8*8)\n",
                "        return self.fc2(F.relu(self.fc1(x)))\n",
                "\n",
                "print(\"Model defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ZENITH BENCHMARK (optimized path)\n",
                "print(\"=\"*50)\n",
                "print(\"ZENITH DATALOADER (Optimized with Prefetch)\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "zenith_loader = zenith.DataLoader(\n",
                "    'data_1gb.parquet',\n",
                "    batch_size=BATCH_SIZE,\n",
                "    shuffle=True,\n",
                "    device=device,\n",
                "    prefetch_factor=4  # Prefetch 4 batches ahead\n",
                ")\n",
                "\n",
                "model = CNN().to(device)\n",
                "opt = optim.Adam(model.parameters())\n",
                "crit = nn.CrossEntropyLoss()\n",
                "\n",
                "z_times = []\n",
                "for ep in range(EPOCHS):\n",
                "    model.train()\n",
                "    t0 = time.time()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for batch in zenith_loader:\n",
                "        # Use optimized to_torch() - single method call\n",
                "        data = batch.to_torch()\n",
                "        \n",
                "        # Reshape features to images\n",
                "        features = data['features']\n",
                "        if features.dim() == 2:\n",
                "            features = features.view(-1, 3, 32, 32)\n",
                "        \n",
                "        x = features.to(device)\n",
                "        y = data['label'].to(device)\n",
                "        \n",
                "        opt.zero_grad()\n",
                "        loss = crit(model(x), y)\n",
                "        loss.backward()\n",
                "        opt.step()\n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    z_times.append(time.time()-t0)\n",
                "    print(f\"Epoch {ep+1}: Loss={total_loss/len(zenith_loader):.4f}, Time={z_times[-1]:.2f}s\")\n",
                "\n",
                "z_avg = sum(z_times[1:])/len(z_times[1:])\n",
                "print(f\"\\nZenith avg: {z_avg:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PYTORCH BENCHMARK\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"PYTORCH DATALOADER\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "pt_loader = TorchDataLoader(\n",
                "    TensorDataset(torch.from_numpy(data_images), torch.from_numpy(data_labels)),\n",
                "    batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True\n",
                ")\n",
                "\n",
                "model = CNN().to(device)\n",
                "opt = optim.Adam(model.parameters())\n",
                "\n",
                "pt_times = []\n",
                "for ep in range(EPOCHS):\n",
                "    model.train()\n",
                "    t0 = time.time()\n",
                "    total_loss = 0\n",
                "    \n",
                "    for x, y in pt_loader:\n",
                "        x, y = x.to(device), y.to(device)\n",
                "        opt.zero_grad()\n",
                "        loss = crit(model(x), y)\n",
                "        loss.backward()\n",
                "        opt.step()\n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    pt_times.append(time.time()-t0)\n",
                "    print(f\"Epoch {ep+1}: Loss={total_loss/len(pt_loader):.4f}, Time={pt_times[-1]:.2f}s\")\n",
                "\n",
                "pt_avg = sum(pt_times[1:])/len(pt_times[1:])\n",
                "print(f\"\\nPyTorch avg: {pt_avg:.2f}s\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RESULTS\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"BENCHMARK RESULTS - 1GB DATASET\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Zenith version: {zenith.__version__}\")\n",
                "print(f\"Dataset: {NUM_SAMPLES:,} samples (~1GB)\")\n",
                "print(f\"Device: {device}\")\n",
                "print(f\"Prefetch: 4 batches\")\n",
                "print(\"-\"*50)\n",
                "print(f\"Zenith:  {z_avg:.2f}s per epoch\")\n",
                "print(f\"PyTorch: {pt_avg:.2f}s per epoch\")\n",
                "print(\"-\"*50)\n",
                "if z_avg < pt_avg:\n",
                "    print(f\"Zenith is {pt_avg/z_avg:.2f}x faster\")\n",
                "else:\n",
                "    print(f\"PyTorch is {z_avg/pt_avg:.2f}x faster\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}