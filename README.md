# Zenith Infrastructure

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](LICENSE)
[![Rust](https://img.shields.io/badge/Rust-1.75%2B-orange.svg)](https://www.rust-lang.org/)
[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
[![PyPI](https://img.shields.io/pypi/v/zenith-ai)](https://pypi.org/project/zenith-ai/)

> **High-Performance Infrastructure for the AI Era**
>
> *"Stop Starving Your GPUs. Feed Them with Zenith."*

---

##  Vision

Zenith is a comprehensive infrastructure ecosystem designed to accelerate AI/ML training and inference at scale. It provides:

1. **Zenith GPU Runtime** - GPU-aware runtime with automatic kernel selection, topology-aware placement, and ZeRO-style memory offload
2. **Zenith Job Scheduler** - Lightweight mini-Slurm with gang scheduling, topology awareness, and preemption support
3. **Zenith CPU Engine** - Ultra-low-latency CPU runtime with NUMA awareness, io_uring, and lock-free data structures

**Zenith doesn't replace PyTorch/DeepSpeed/Triton** â€” it provides the runtime performance layer and lightweight scheduler that accelerates and orchestrates large AI workloads on real infrastructure.

---

##  Performance

| Metric | Standard | Zenith | Improvement |
|--------|----------|--------|-------------|
| Data Loading | 50K events/s | 6M events/s | **120x** |
| Latency (P99) | 10 ms | 100 Âµs | **100x** |
| Memory Overhead | 2.5 GB | 150 MB | **16x less** |
| GPU Utilization | 60-70% | 95%+ | **+35%** |

---

##  Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ZENITH ECOSYSTEM                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  GPU Runtime    â”‚  â”‚  Job Scheduler  â”‚  â”‚   CPU Engine        â”‚  â”‚
â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚  â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚  â”‚
â”‚  â”‚  â€¢ Kernel Mgr   â”‚  â”‚  â€¢ Gang Sched   â”‚  â”‚   â€¢ NUMA Aware      â”‚  â”‚
â”‚  â”‚  â€¢ Memory       â”‚  â”‚  â€¢ Topology     â”‚  â”‚   â€¢ io_uring        â”‚  â”‚ 
â”‚  â”‚  â€¢ NCCL         â”‚  â”‚  â€¢ Preemption   â”‚  â”‚   â€¢ Lock-free       â”‚  â”‚
â”‚  â”‚  â€¢ ZeRO Offload â”‚  â”‚  â€¢ Quotas       â”‚  â”‚   â€¢ Ring Buffers    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    Python SDK (pip install zenith-ai)           â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚â”‚
â”‚  â”‚  â”‚ PyTorch       â”‚  â”‚ TensorFlow    â”‚  â”‚ JAX (planned)     â”‚    â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

##  Quick Start

### Installation

```bash
# Python SDK (recommended)
pip install zenith-ai

# From source
git clone https://github.com/vibeswithkk/Zenith-dataplane.git
cd Zenith-dataplane
cargo build --release
```

### PyTorch Example

```python
import zenith.torch as zt

# Create high-performance data loader
loader = zt.DataLoader(
    source="path/to/training_data",
    batch_size=64,
    shuffle=True,
    preprocessing_plugin="image_resize.wasm",
    num_workers=4,
    pin_memory=True
)

# Training loop - GPU never starves for data
for epoch in range(10):
    for batch in loader:
        outputs = model(batch)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
```

### TensorFlow Example

```python
import zenith.tensorflow as ztf

dataset = ztf.ZenithDataset(
    source="path/to/training_data",
    preprocessing_plugin="image_resize.wasm"
)

dataset = dataset.batch(32).prefetch(tf.data.AUTOTUNE)
model.fit(dataset, epochs=10)
```

---

##  Repository Structure

```
zenith/
â”œâ”€â”€ zenith-runtime-gpu/     # GPU optimization runtime
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ device.rs       # GPU topology discovery
â”‚   â”‚   â”œâ”€â”€ kernel.rs       # Kernel manager
â”‚   â”‚   â”œâ”€â”€ memory.rs       # ZeRO-style offload
â”‚   â”‚   â””â”€â”€ collective.rs   # NCCL integration
â”‚   â””â”€â”€ Cargo.toml
â”‚
â”œâ”€â”€ zenith-scheduler/       # Job scheduler (mini-Slurm)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ job.rs          # Job definitions
â”‚   â”‚   â”œâ”€â”€ node.rs         # Node registry
â”‚   â”‚   â”œâ”€â”€ scheduler.rs    # Gang scheduling
â”‚   â”‚   â””â”€â”€ api/            # gRPC & REST APIs
â”‚   â””â”€â”€ Cargo.toml
â”‚
â”œâ”€â”€ zenith-runtime-cpu/     # CPU low-latency runtime
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ buffer.rs       # Lock-free ring buffers
â”‚   â”‚   â”œâ”€â”€ numa.rs         # NUMA topology
â”‚   â”‚   â”œâ”€â”€ allocator.rs    # NUMA-aware allocator
â”‚   â”‚   â””â”€â”€ io.rs           # io_uring integration
â”‚   â””â”€â”€ Cargo.toml
â”‚
â”œâ”€â”€ zenith-proto/           # Protocol definitions
â”‚   â””â”€â”€ zenith.proto        # gRPC/Protobuf schemas
â”‚
â”œâ”€â”€ zenith-bench/           # Benchmark harness
â”‚   â””â”€â”€ src/main.rs         # MLPerf-style benchmarks
â”‚
â”œâ”€â”€ sdk-python/             # Python SDK
â”‚   â”œâ”€â”€ zenith/             # Python package
â”‚   â””â”€â”€ pyproject.toml      # Maturin config
â”‚
â””â”€â”€ docs/                   # Documentation
    â”œâ”€â”€ ARCHITECTURE.md     # System architecture
    â”œâ”€â”€ PYTORCH_GUIDE.md    # PyTorch integration
    â””â”€â”€ PLUGIN_GUIDE.md     # WASM plugin development
```

---

##  Development

### Prerequisites

- Rust 1.75+
- Python 3.10+
- (Optional) CUDA Toolkit 11.8+ for GPU features
- (Optional) NCCL for multi-GPU communication

### Building

```bash
# Build all crates
cargo build --release

# Run tests
cargo test --all

# Run benchmarks
cargo run -p zenith-bench --release -- full
```

### Python Development

```bash
cd sdk-python

# Create virtual environment
python -m venv .venv
source .venv/bin/activate

# Install with maturin
pip install maturin
maturin develop

# Run tests
pytest tests/
```

---

## ğŸ“– Documentation

| Document | Description |
|----------|-------------|
| [Architecture](docs/ARCHITECTURE.md) | System design and components |
| [PyTorch Guide](docs/PYTORCH_GUIDE.md) | PyTorch integration tutorial |
| [TensorFlow Guide](docs/TENSORFLOW_GUIDE.md) | TensorFlow integration tutorial |
| [Plugin Guide](docs/PLUGIN_GUIDE.md) | WASM plugin development |
| [Operator Guide](docs/OPERATOR_GUIDE.md) | Cluster deployment |
| [API Reference](docs/API.md) | gRPC and REST API |

---

## ğŸ”¬ Technical References

This project is built upon research and industry best practices:

1. **ZeRO: Memory Optimizations** - Microsoft DeepSpeed ([arXiv](https://arxiv.org/abs/1910.02054))
2. **NVIDIA NCCL** - Collective Communications ([docs](https://docs.nvidia.com/deeplearning/nccl/))
3. **Slurm Workload Manager** - Gang Scheduling ([SchedMD](https://slurm.schedmd.com/))
4. **io_uring** - Linux Async I/O ([man7.org](https://man7.org/linux/man-pages/man7/io_uring.7.html))
5. **Apache Arrow** - Zero-Copy Data ([arrow.apache.org](https://arrow.apache.org/))
6. **MLPerf** - Benchmark Standards ([mlcommons.org](https://mlcommons.org/))

---

##  Author

**Wahyu Ardiansyah** ğŸ‡®ğŸ‡©

- GitHub: [@vibeswithkk](https://github.com/vibeswithkk)
- Made with  in Indonesia

---

##  License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

```
Copyright 2025 Wahyu Ardiansyah and Zenith AI Contributors
```

---

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

---

## â­ Star History

If you find Zenith useful, please consider giving it a star on GitHub!
